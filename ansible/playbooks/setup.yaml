---
- name: Add a new user with sudo privileges
  hosts: remote
  vars_files:
    - ../vars.yaml
  become: true
  tasks:
    - name: Create a new user
      user:
        name: "{{ linux_username }}"
        password: "{{ linux_password | password_hash('sha512') }}"
        shell: /bin/bash
        state: present
        create_home: yes
        groups:
          - sudo
        append: yes

    - name: Allow new user to have passwordless sudo
      lineinfile:
        dest: /etc/sudoers
        state: present
        regexp: '^%{{ linux_username }}'
        line: '%{{ linux_username }} ALL=(ALL) NOPASSWD: ALL'
        validate: 'visudo -cf %s'

- name: Install Ansible/pip dependencies on leader machine
  hosts: leader
  vars_files:
    - ../vars.yaml
  become: true
  tasks:
    - name: Check if pip is installed
      command: python3 -m pip -V
      register: pip_installed
      ignore_errors: true

    - name: Download pip installer
      get_url:
        url: https://bootstrap.pypa.io/get-pip.py
        dest: /tmp/get-pip.py
        mode: '0644'
      when: pip_installed.failed

    - name: Run pip installer
      command: python3 /tmp/get-pip.py --user
      when: pip_installed.failed

    - name: Ensure python3-venv is installed
      apt:
        name: python3-venv
        state: present
      when: ansible_os_family == 'Debian'

    - name: Install packaging package
      pip:
        name:
          - packaging
          - virtualenv

    - name: Create virtual environment
      become_user: root
      command:
        cmd: python3 -m venv {{ venv_path }}
        creates: "{{ venv_path }}"

    - name: Install pre-requisites
      pip:
        name:
          - openshift
          - pyyaml
          - kubernetes
        virtualenv: "{{ venv_path }}"
        virtualenv_site_packages: true

- name: Check kubectl is present on leader node
  hosts: leader
  vars_files:
    - ../vars.yaml
  become: yes
  tasks:
    - name: Check if kubectl is installed and connected to cluster
      command: kubectl cluster-info
      register: kubectl_installed
      ignore_errors: true

    - name: Fail playbook
      fail:
        msg: kubectl not installed or not connected with a cluster on leader node. Can't continue playbook without a present k8s deployment.
      when: kubectl_installed.rc != 0

    - name: Ensure .kube directory exists for user {{ linux_username }}
      file:
        path: /home/{{ linux_username }}/.kube
        state: directory
        owner: "{{ linux_username }}"
        group: "{{ linux_username }}"
        mode: '0755'

    - name: Copy kubeconfig to {{ linux_username }}'s home directory
      copy:
        remote_src: true
        src: /etc/kubernetes/admin.conf
        dest: /home/{{ linux_username }}/.kube/config
        owner: "{{ linux_username }}"
        group: "{{ linux_username }}"
        mode: '0644'

- name: Install helm on leader node
  hosts: leader
  vars_files:
    - ../vars.yaml
  tasks:
    - name: Check if helm is installed
      command: which helm
      register: helm_installed
      ignore_errors: true

    - name: Download Helm installation script
      get_url:
        url: https://raw.githubusercontent.com/helm/helm/master/scripts/get
        dest: /tmp/get_helm.sh
        mode: '0755'
      when: helm_installed.failed
      become: yes

    - name: Install Helm
      command: /tmp/get_helm.sh -v "v{{ helm_version }}"
      when: helm_installed.failed
      become: yes

    - name: Remove Helm installation script
      file:
        path: /tmp/get_helm.sh
        state: absent
      when: helm_installed.failed
      become: yes

# to be on the save side and prevent any manual managing effort, we'll create more PV then actually required
- name: Create bind-mount directories for PersistentVolumes on all nodes
  hosts: remote
  become: yes
  vars_files:
    - ../vars.yaml
  tasks:
    - name: Create MongoDB directory
      file:
        path: "/mnt/disks/mongodb-data-{{ ansible_hostname | lower }}"
        state: directory
        mode: '0755'
      loop: "{{ range(1, replica_count + 1) }}"

    - name: Create Zookeeper data source directory if it does not exist
      file:
        path: "/mnt/disks/zookeeper-data-{{ ansible_hostname | lower }}"
        state: directory
        mode: '0755'

    - name: Create HDFS data source directory if it does not exist
      file:
        path: "/mnt/disks/hdfs-data-{{ ansible_hostname | lower }}-{{ item }}"
        state: directory
        mode: '0755'
      loop: "{{ range(1, data_node_replicas + name_node_replicas + journal_node_replicas + 1) }}"

    - name: Create MongoDB target directory if it does not exist
      file:
        path: "/Users/{{ linux_username }}/mongodb-data-{{ ansible_hostname | lower }}"
        state: directory
        mode: '0755'
      loop: "{{ range(1, replica_count + 1) }}"

    - name: Create Zookeeper data target directory if it does not exist
      file:
        path: "/Users/{{ linux_username }}/zookeeper-data-{{ ansible_hostname | lower }}"
        state: directory
        mode: '0755'

    - name: Create HDFS data target directory if it does not exist
      file:
        path: "/Users/{{ linux_username }}/hdfs-data-{{ ansible_hostname | lower }}-{{ item }}"
        state: directory
        mode: '0755'
      loop: "{{ range(1, data_node_replicas + name_node_replicas + journal_node_replicas + 1) }}"

    - name: Bind mount MongoDB directories
      ansible.posix.mount:
        src: "/Users/{{ linux_username }}/mongodb-data-{{ ansible_hostname | lower }}"
        path: "/mnt/disks/mongodb-data-{{ ansible_hostname | lower }}"
        fstype: none
        opts: bind
        state: mounted
      loop: "{{ range(1, replica_count + 1) }}"

    - name: Bind mount zookeeper directories
      ansible.posix.mount:
        src: "/Users/{{ linux_username }}/zookeeper-data-{{ ansible_hostname | lower }}"
        path: "/mnt/disks/zookeeper-data-{{ ansible_hostname | lower }}"
        fstype: none
        opts: bind
        state: mounted

    - name: Bind mount hdfs directories
      ansible.posix.mount:
        src: "/Users/{{ linux_username }}/hdfs-data-{{ ansible_hostname | lower }}-{{ item }}"
        path: "/mnt/disks/hdfs-data-{{ ansible_hostname | lower }}-{{ item }}"
        fstype: none
        opts: bind
        state: mounted
      loop: "{{ range(1, data_node_replicas + name_node_replicas + journal_node_replicas + 1) }}"

- name: Template HDFS cluster charts
  hosts: leader
  vars_files:
    - ../vars.yaml
  vars:
    local_charts_dir: "{{ lookup('env', 'PWD') }}/ansible/charts/"
    remote_charts_dir: /Users/{{ linux_username }}/tmp/charts
  become: true
  tasks:
    - name: Create a temporary directory for templating
      file:
        path: "{{ remote_charts_dir }}"
        state: directory
        mode: '0755'

    - name: Template the Zookeeper cluster file
      template:
        src: "{{ local_charts_dir }}/zookeeper-cluster.yaml"
        dest: "{{ remote_charts_dir }}/zookeeper-cluster.yaml"

    - name: Template the Zookeeper ZNode file
      template:
        src: "{{ local_charts_dir }}/zookeeper-znode.yaml"
        dest: "{{ remote_charts_dir }}/zookeeper-znode.yaml"

    - name: Template the HDFS cluster file
      template:
        src: "{{ local_charts_dir }}/hdfs-cluster.yaml"
        dest: "{{ remote_charts_dir }}/hdfs-cluster.yaml"

- name: Deploy spark-operator helm chart
  hosts: leader
  vars_files:
    - ../vars.yaml
  become: yes
  tasks:
    - name: Add spark-operator repository
      kubernetes.core.helm_repository:
        name: spark-operator
        repo_url: https://kubeflow.github.io/spark-operator

    - name: Deploy spark-operator helm chart
      kubernetes.core.helm:
        name: spark-operator
        chart_ref: spark-operator/spark-operator
        chart_version: "{{ spark_operator_version }}"
        release_namespace: "{{ k8s_namespace }}"
        atomic: true
        wait: true

- name: Deploy MongoDB replica set
  hosts: leader
  vars_files:
    - ../vars.yaml
  become: yes
  tasks:
    - name: Deploy mongodb helm chart
      kubernetes.core.helm:
        name: mongodb
        chart_ref: oci://registry-1.docker.io/bitnamicharts/mongodb
        chart_version: "{{ mongo_db_chart_version }}"
        release_namespace: "{{ k8s_namespace }}"
        atomic: true
        wait: true
        values:
          architecture: replicaset
          persistence:
            storageClass: "{{ storage_class }}"
          replicaCount: "{{ replica_count }}"
          auth:
            usernames:
              - "{{ mongodb_username }}"
            passwords:
              - "{{ mongodb_password }}"
            databases:
              - "{{ mongodb_database }}"
          arbiter:
            resourcesPreset: "{{ mongodb_resource_preset }}"
          externalAccess:
            enabled: true
            service:
              type: NodePort
              nodePorts[0]: 27017

- name: Install HDFS Operator's
  hosts: leader
  vars_files:
    - ../vars.yaml
  vars:
    ansible_python_interpreter: "{{ venv_path }}/bin/python3"
  become: yes
  become_user: root
  tasks:
    - name: Add stackable-stable repository
      kubernetes.core.helm_repository:
        name: stackable-stable
        repo_url: https://repo.stackable.tech/repository/helm-stable/

    - name: Deploy Zookeeper operator helm chart
      kubernetes.core.helm:
        name: zookeeper-operator
        chart_ref: stackable-stable/zookeeper-operator
        chart_version: "{{ stackable_operator_version }}"
        release_namespace: "{{ k8s_namespace }}"
        atomic: true
        wait: true

    - name: Deploy HDFS operator helm chart
      kubernetes.core.helm:
        name: hdfs-operator
        chart_ref: stackable-stable/hdfs-operator
        chart_version: "{{ stackable_operator_version }}"
        release_namespace: "{{ k8s_namespace }}"
        atomic: true
        wait: true

    - name: Deploy Commons operator helm chart
      kubernetes.core.helm:
        name: commons-operator
        chart_ref: stackable-stable/commons-operator
        chart_version: "{{ stackable_operator_version }}"
        release_namespace: "{{ k8s_namespace }}"
        atomic: true
        wait: true

    - name: Deploy Secret operator helm chart
      kubernetes.core.helm:
        name: secret-operator
        chart_ref: stackable-stable/secret-operator
        chart_version: "{{ stackable_operator_version }}"
        release_namespace: "{{ k8s_namespace }}"
        atomic: true
        wait: true

    - name: Deploy Listener operator helm chart
      kubernetes.core.helm:
        name: listener-operator
        chart_ref: stackable-stable/listener-operator
        chart_version: "{{ stackable_operator_version }}"
        release_namespace: "{{ k8s_namespace }}"
        atomic: true
        wait: true

- name: Deploy HDFS cluster charts
  hosts: leader
  vars_files:
    - ../vars.yaml
  vars:
    remote_charts_dir: /Users/{{ linux_username }}/tmp/charts
    ansible_python_interpreter: "{{ venv_path }}/bin/python3"
  become: yes
  tasks:
    - name: Deploy Zookeeper chart
      kubernetes.core.k8s:
        name: zookeeper
        src: "{{ remote_charts_dir }}/zookeeper-cluster.yaml"
        namespace: "{{ k8s_namespace }}"
        state: present
        wait: true

    - name: Deploy Zookeeper ZNode chart
      kubernetes.core.k8s:
        name: zookeeper-znode
        src: "{{ remote_charts_dir }}/zookeeper-znode.yaml"
        namespace: "{{ k8s_namespace }}"
        state: present
        wait: true

    - name: Deploy HDFS chart
      kubernetes.core.k8s:
        name: hdfs
        src: "{{ remote_charts_dir }}/hdfs-cluster.yaml"
        namespace: "{{ k8s_namespace }}"
        state: present
        wait: true

    - name: Print info message
      debug:
        msg: HDFS will need some time to set the all nodes up. You may want to watch the cluster and check if all pods are up.

- import_playbook: facts.yaml
