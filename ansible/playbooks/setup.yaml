---
- name: Add a new user with sudo privileges
  hosts: remote
  vars_files:
    - ../vars.yaml
  become: true
  tasks:
    - name: Create a new user
      user:
        name: "{{ linux_username }}"
        password: "{{ linux_password | password_hash('sha512') }}"
        shell: /bin/bash
        state: present
        create_home: yes
        groups:
          - sudo
        append: yes

    - name: Allow new user to have passwordless sudo
      lineinfile:
        dest: /etc/sudoers
        state: present
        regexp: '^%{{ linux_username }}'
        line: '%{{ linux_username }} ALL=(ALL) NOPASSWD: ALL'
        validate: 'visudo -cf %s'

    - name: Install acl to fix a tmp-file creation issue when running ansible tasks as non-root user
      apt:
        name: acl
        state: present

- name: Install Ansible/pip dependencies on leader machine
  hosts: leader
  vars_files:
    - ../vars.yaml
  become: true
  tasks:
    - name: Check if pip is installed
      command: python3 -m pip -V
      register: pip_installed
      ignore_errors: true

    - name: Download pip installer
      get_url:
        url: https://bootstrap.pypa.io/get-pip.py
        dest: /tmp/get-pip.py
        mode: '0644'
      when: pip_installed.failed

    - name: Run pip installer
      command: python3 /tmp/get-pip.py --user
      when: pip_installed.failed

    - name: Ensure python3-venv is installed
      apt:
        name: python3-venv
        state: present
        force_apt_get: true
        update_cache: true

    - name: Install pip packages
      pip:
        name:
          - packaging
          - virtualenv
          - jinja2
          - pathlib

    - name: Create virtual environment
      become_user: root
      command:
        cmd: python3 -m venv {{ venv_path }}
        creates: "{{ venv_path }}"

    - name: Install pre-requisites
      pip:
        name:
          - openshift
          - pyyaml
          - kubernetes
        virtualenv: "{{ venv_path }}"
        virtualenv_site_packages: true

- name: Check kubectl is present on leader node
  hosts: leader
  vars_files:
    - ../vars.yaml
  become: true
  tasks:
    - name: Check if kubectl is installed and connected to cluster
      command: kubectl cluster-info
      register: kubectl_installed
      ignore_errors: true

    - name: Fail playbook
      fail:
        msg: kubectl not installed or not connected with a cluster on leader node. Can't continue playbook without a present k8s deployment.
      when: kubectl_installed.rc != 0

    - name: Ensure .kube directory exists for user {{ linux_username }}
      file:
        path: /home/{{ linux_username }}/.kube
        state: directory
        owner: "{{ linux_username }}"
        group: "{{ linux_username }}"
        mode: '0755'

    - name: Copy kubeconfig to {{ linux_username }}'s home directory
      copy:
        remote_src: true
        src: /etc/kubernetes/admin.conf
        dest: /home/{{ linux_username }}/.kube/config
        owner: "{{ linux_username }}"
        group: "{{ linux_username }}"
        mode: '0644'

- name: Install helm on leader node
  hosts: leader
  vars_files:
    - ../vars.yaml
  tasks:
    - name: Check if helm is installed
      command: which helm
      register: helm_installed
      ignore_errors: true

    - name: Download Helm installation script
      get_url:
        url: https://raw.githubusercontent.com/helm/helm/master/scripts/get
        dest: /tmp/get_helm.sh
        mode: '0755'
      when: helm_installed.failed
      become: true

    - name: Install Helm
      command: /tmp/get_helm.sh -v "v{{ helm_version }}"
      when: helm_installed.failed
      become: true

    - name: Remove Helm installation script
      file:
        path: /tmp/get_helm.sh
        state: absent
      when: helm_installed.failed
      become: true

# to be on the save side and prevent any manual managing effort, we'll create more PV then actually required
- name: Create bind-mount directories for PersistentVolumes on all nodes
  hosts: remote
  become: true
  vars_files:
    - ../vars.yaml
  tasks:
    - name: Create MongoDB directory
      file:
        path: "/mnt/disks/mongodb-data-{{ ansible_hostname | lower }}"
        state: directory
        mode: '0755'
      loop: "{{ range(1, replica_count + 1) }}"

    - name: Create Zookeeper data source directory if it does not exist
      file:
        path: "/mnt/disks/zookeeper-data-{{ ansible_hostname | lower }}"
        state: directory
        mode: '0755'

    - name: Create HDFS data source directory if it does not exist
      file:
        path: "/mnt/disks/hdfs-data-{{ ansible_hostname | lower }}-{{ item }}"
        state: directory
        mode: '0755'
      loop: "{{ range(1, data_node_replicas + name_node_replicas + journal_node_replicas + 1) }}"

    - name: Create MongoDB target directory if it does not exist
      file:
        path: "/Users/{{ linux_username }}/mongodb-data-{{ ansible_hostname | lower }}"
        state: directory
        mode: '0755'
      loop: "{{ range(1, replica_count + 1) }}"

    - name: Create Zookeeper data target directory if it does not exist
      file:
        path: "/Users/{{ linux_username }}/zookeeper-data-{{ ansible_hostname | lower }}"
        state: directory
        mode: '0755'

    - name: Create HDFS data target directory if it does not exist
      file:
        path: "/Users/{{ linux_username }}/hdfs-data-{{ ansible_hostname | lower }}-{{ item }}"
        state: directory
        mode: '0755'
      loop: "{{ range(1, data_node_replicas + name_node_replicas + journal_node_replicas + 1) }}"

    - name: Bind mount MongoDB directories
      ansible.posix.mount:
        src: "/Users/{{ linux_username }}/mongodb-data-{{ ansible_hostname | lower }}"
        path: "/mnt/disks/mongodb-data-{{ ansible_hostname | lower }}"
        fstype: none
        opts: bind
        state: mounted
      loop: "{{ range(1, replica_count + 1) }}"

    - name: Bind mount zookeeper directories
      ansible.posix.mount:
        src: "/Users/{{ linux_username }}/zookeeper-data-{{ ansible_hostname | lower }}"
        path: "/mnt/disks/zookeeper-data-{{ ansible_hostname | lower }}"
        fstype: none
        opts: bind
        state: mounted

    - name: Bind mount hdfs directories
      ansible.posix.mount:
        src: "/Users/{{ linux_username }}/hdfs-data-{{ ansible_hostname | lower }}-{{ item }}"
        path: "/mnt/disks/hdfs-data-{{ ansible_hostname | lower }}-{{ item }}"
        fstype: none
        opts: bind
        state: mounted
      loop: "{{ range(1, data_node_replicas + name_node_replicas + journal_node_replicas + 1) }}"

- name: Template HDFS cluster charts
  hosts: leader
  vars_files:
    - ../vars.yaml
  become: true
  tasks:
    - name: Create a temporary directory for templating
      file:
        path: "{{ remote_template_dir }}"
        state: directory
        mode: '0755'

    - name: Template the Zookeeper cluster file
      template:
        src: "{{ local_template_dir }}/zookeeper-cluster.yaml"
        dest: "{{ remote_template_dir }}/zookeeper-cluster.yaml"

    - name: Template the Zookeeper ZNode file
      template:
        src: "{{ local_template_dir }}/zookeeper-znode.yaml"
        dest: "{{ remote_template_dir }}/zookeeper-znode.yaml"

    - name: Template the HDFS cluster file
      template:
        src: "{{ local_template_dir }}/hdfs-cluster.yaml"
        dest: "{{ remote_template_dir }}/hdfs-cluster.yaml"

    - name: Copy webhdfs.yaml to leader
      template:
        src: "{{ local_template_dir }}/webhdfs.yaml"
        dest: "{{ remote_template_dir }}/webhdfs.yaml"

- name: Deploy spark-operator helm chart
  hosts: leader
  vars_files:
    - ../vars.yaml
  become: true
  tasks:
    - name: Add spark-operator repository
      kubernetes.core.helm_repository:
        name: spark-operator
        repo_url: https://kubeflow.github.io/spark-operator

    - name: Deploy spark-operator helm chart
      kubernetes.core.helm:
        name: spark-operator
        chart_ref: spark-operator/spark-operator
        chart_version: "{{ spark_operator_version }}"
        release_namespace: "{{ k8s_namespace }}"
        atomic: true
        wait: true

- name: Deploy MongoDB replica set
  hosts: leader
  vars_files:
    - ../vars.yaml
  become: true
  tasks:
    - name: Deploy mongodb helm chart
      kubernetes.core.helm:
        name: mongodb
        chart_ref: oci://registry-1.docker.io/bitnamicharts/mongodb
        chart_version: "{{ mongo_db_chart_version }}"
        release_namespace: "{{ k8s_namespace }}"
        atomic: true
        wait: true
        values:
          architecture: replicaset
          replicaSetName: "{{ replica_set_name }}"
          persistence:
            storageClass: "{{ storage_class }}"
          replicaCount: "{{ replica_count }}"
          auth:
            rootUser: "{{ mongodb_root_username }}"
            usernames:
              - "{{ mongodb_username }}"
            passwords:
              - "{{ mongodb_password }}"
            databases:
              - "{{ mongodb_database }}"
          arbiter:
            resourcesPreset: "{{ mongodb_resource_preset }}"

- name: Install HDFS Operator's
  hosts: leader
  vars_files:
    - ../vars.yaml
  vars:
    ansible_python_interpreter: "{{ venv_path }}/bin/python3"
  become: true
  become_user: root
  tasks:
    - name: Add stackable-stable repository
      kubernetes.core.helm_repository:
        name: stackable-stable
        repo_url: https://repo.stackable.tech/repository/helm-stable/

    - name: Deploy Zookeeper operator helm chart
      kubernetes.core.helm:
        name: zookeeper-operator
        chart_ref: stackable-stable/zookeeper-operator
        chart_version: "{{ stackable_operator_version }}"
        release_namespace: "{{ k8s_namespace }}"
        atomic: true
        wait: true

    - name: Deploy HDFS operator helm chart
      kubernetes.core.helm:
        name: hdfs-operator
        chart_ref: stackable-stable/hdfs-operator
        chart_version: "{{ stackable_operator_version }}"
        release_namespace: "{{ k8s_namespace }}"
        atomic: true
        wait: true

    - name: Deploy Commons operator helm chart
      kubernetes.core.helm:
        name: commons-operator
        chart_ref: stackable-stable/commons-operator
        chart_version: "{{ stackable_operator_version }}"
        release_namespace: "{{ k8s_namespace }}"
        atomic: true
        wait: true

    - name: Deploy Secret operator helm chart
      kubernetes.core.helm:
        name: secret-operator
        chart_ref: stackable-stable/secret-operator
        chart_version: "{{ stackable_operator_version }}"
        release_namespace: "{{ k8s_namespace }}"
        atomic: true
        wait: true

    - name: Deploy Listener operator helm chart
      kubernetes.core.helm:
        name: listener-operator
        chart_ref: stackable-stable/listener-operator
        chart_version: "{{ stackable_operator_version }}"
        release_namespace: "{{ k8s_namespace }}"
        atomic: true
        wait: true

- name: Deploy HDFS cluster charts
  hosts: leader
  vars_files:
    - ../vars.yaml
  vars:
    ansible_python_interpreter: "{{ venv_path }}/bin/python3"
  become: true
  tasks:
    - name: Deploy Zookeeper chart
      kubernetes.core.k8s:
        name: zookeeper
        src: "{{ remote_template_dir }}/zookeeper-cluster.yaml"
        namespace: "{{ k8s_namespace }}"
        state: present
        wait: true

    - name: Deploy Zookeeper ZNode chart
      kubernetes.core.k8s:
        name: zookeeper-znode
        src: "{{ remote_template_dir }}/zookeeper-znode.yaml"
        namespace: "{{ k8s_namespace }}"
        state: present
        wait: true

    - name: Deploy HDFS chart
      kubernetes.core.k8s:
        name: hdfs
        src: "{{ remote_template_dir }}/hdfs-cluster.yaml"
        namespace: "{{ k8s_namespace }}"
        state: present
        wait: true

    - name: Deploy WebHDFS application
      kubernetes.core.k8s:
        name: webhdfs
        namespace: "{{ k8s_namespace }}"
        src: "{{ remote_template_dir }}/webhdfs.yaml"
        state: present
        wait: true

- name: Wait for HDFS to be ready
  hosts: leader
  vars_files:
    - ../vars.yaml
  vars:
    ansible_python_interpreter: "{{ venv_path }}/bin/python3"
  become: true
  tasks:
    - name: Verify HDFS NameNode pods are ready
      k8s_info:
        namespace: "{{ k8s_namespace }}"
        kind: Pod
        label_selectors:
          - 'role=namenode'
      register: namenode_pods
      retries: 30
      delay: 10
      until: namenode_pods.resources | map(attribute='status.containerStatuses') | map('map', attribute='ready') | flatten | reject('equalto', true) | list | length == 0

    - name: Verify HDFS DataNode pods are ready
      k8s_info:
        namespace: "{{ k8s_namespace }}"
        kind: Pod
        label_selectors:
          - 'role=datanode'
      register: datanode_pods
      retries: 30
      delay: 10
      until: datanode_pods.resources | map(attribute='status.containerStatuses') | map('map', attribute='ready') | flatten | reject('equalto', true) | list | length == 0

- name: Copy custom SparkListener to HDFS
  hosts: leader
  become: true
  vars_files:
    - ../vars.yaml
  vars:
    ansible_python_interpreter: "{{ venv_path }}/bin/python3"
  tasks:
    - name: Copy the JAR file to remote machine
      synchronize:
        src: "{{ lookup('env', 'PWD') }}/ansible/jars/{{ listener_jar }}"
        dest: "/tmp/{{ listener_jar }}"
        delete: yes

    - name: Copy jar local file to webhdfs pod
      kubernetes.core.k8s_cp:
        namespace: "{{ k8s_namespace }}"
        pod: webhdfs-0
        local_path: "/tmp/{{ listener_jar }}"
        remote_path: "/tmp/{{ listener_jar }}"

    - name: Issue initial PUT command to create file in HDFS
      kubernetes.core.k8s_exec:
        namespace: "{{ k8s_namespace }}"
        pod: webhdfs-0
        command: "curl -s -XPUT -T /tmp/{{ listener_jar }} \"http://{{ hdfs_cluster_name }}-namenode-default-0.{{ hdfs_cluster_name }}-namenode-default.default.svc.cluster.local:9870/webhdfs/v1/{{ listener_jar }}?user.name=stackable&op=CREATE&noredirect=true\""
      register: put_result

    - name: Extract location URL from PUT response
      set_fact:
        location_url: "{{ put_result.stdout | from_json() | json_query('Location') }}"

    - name: Issue second PUT command to complete file creation
      kubernetes.core.k8s_exec:
        namespace: "{{ k8s_namespace }}"
        pod: webhdfs-0
        command: "curl -s -XPUT -T /tmp/{{ listener_jar }} {{ location_url }}"

    - name: Recheck status to verify file creation
      kubernetes.core.k8s_exec:
        namespace: "{{ k8s_namespace }}"
        pod: webhdfs-0
        command: "curl -s -XGET \"http://{{ hdfs_cluster_name }}-namenode-default-0.{{ hdfs_cluster_name }}-namenode-default.default.svc.cluster.local:9870/webhdfs/v1/?op=LISTSTATUS\""
      register: status_result

    - name: Parse status_result stdout as JSON
      set_fact:
        file_status: "{{ status_result.stdout | from_json }}"

    - name: Verify the file_status contains the created file
      assert:
        that:
          - file_status.FileStatuses.FileStatus | length > 0
          - file_status.FileStatuses.FileStatus[0].pathSuffix == "{{ listener_jar }}"
          - file_status.FileStatuses.FileStatus[0].length > 0

- name: Prepare HiBench
  hosts: leader
  vars_files:
    - ../vars.yaml
  become: true
  tasks:
    - name: Setup dependencies
      block:
        - name: Install apt dependencies
          apt:
            pkg:
              - ssh
              - pdsh
              - bc
              - python2
              - openjdk-{{ java_version }}-jdk
              - maven
            state: present

        - name: Verify Python2 installation
          command: which python2
          register: python_check
          failed_when: python_check.rc != 0

        - name: Verify Java installation
          command: which java
          register: java_check
          failed_when: java_check.rc != 0

        - name: Verify Maven installation
          command: which mvn
          register: maven_check
          failed_when: maven_check.rc != 0

        - name: Set main Java version to {{ java_version }}
          shell: update-alternatives --set java /usr/lib/jvm/java-{{ java_version }}-openjdk-amd64/jre/bin/java

        - name: Add JAVA_HOME to .bashrc
          lineinfile:
            path: "{{ user_dir }}/.bashrc"
            line: 'export JAVA_HOME=$(readlink -f $(which java) | sed "s:bin/java::")'
            state: present

        - name: Add JAVA_HOME to PATH in .bashrc
          lineinfile:
            path: "{{ user_dir }}/.bashrc"
            line: 'export PATH=$JAVA_HOME/bin:$PATH'
            state: present

        - name: Generate an SSH key pair if it doesn't exist
          openssh_keypair:
            path: "{{ user_dir }}/.ssh/id_rsa"
            type: rsa
            passphrase: ''
            comment: 'generated by ansible'
            owner: "{{ linux_username }}"
          register: ssh_key

        - name: Ensure the public key is in the authorized_keys file
          lineinfile:
            path: "{{ user_dir }}/.ssh/authorized_keys"
            line: "{{ ssh_key.public_key }}"
            create: yes
            mode: '0600'

        - name: Ensure correct permissions for authorized_keys
          file:
            path: "{{ user_dir }}/.ssh/authorized_keys"
            mode: '0600'
            owner: "{{ linux_username }}"
            group: "{{ linux_username }}"

    - name: Setup Hadoop
      block:
        - name: Download Hadoop tarball
          get_url:
            url: "https://dlcdn.apache.org/hadoop/common/hadoop-{{ hadoop_version }}/hadoop-{{ hadoop_version }}.tar.gz"
            dest: /tmp/hadoop.tar.gz

        - name: Create Hadoop installation directory
          file:
            path: "{{ hadoop_dir }}"
            state: directory
            mode: '0755'
            owner: "{{ linux_username }}"
            group: "{{ linux_username }}"
            recurse: yes

        - name: Extract Hadoop tarball
          unarchive:
            src: /tmp/hadoop.tar.gz
            dest: "{{ hadoop_dir }}"
            remote_src: yes
            extra_opts: [ --strip-components=1 ]
            # this could be any file nested in hadoop_dir which is created after extracting successfully
            creates: "{{ hadoop_dir }}/etc/hadoop/hadoop-env.sh"

        - name: Set JAVA_HOME in hadoop-env.sh
          lineinfile:
            path: "{{ hadoop_dir }}/etc/hadoop/hadoop-env.sh"
            regexp: '^export JAVA_HOME='
            line: 'export JAVA_HOME=$(readlink -f $(which java) | sed "s:bin/java::")'
            state: present

        - name: Template *-site.xml files
          template:
            src: "{{ hibench_template_dir }}/{{ item }}"
            dest: "{{ hadoop_dir }}/etc/hadoop/{{ item }}"
          loop:
            - core-site.xml
            - hdfs-site.xml
            - mapred-site.xml
            - yarn-site.xml

        - name: Template setup script
          template:
            src: "{{ hibench_template_dir }}/start-deamons.sh"
            dest: "{{ hadoop_dir }}/start-deamons.sh"

        - name: Ensure hadoop logs directory has correct permissions
          file:
            path: "{{ hadoop_dir }}/logs"
            owner: "{{ linux_username }}"
            group: "{{ linux_username }}"
            mode: '0755'
            state: directory

        - name: Template setup script
          template:
            src: "{{ hibench_template_dir }}/start-deamons.sh"
            dest: "{{ hadoop_dir }}/start-deamons.sh"

        - name: Start HDFS and YARN
          become_user: "{{ linux_username }}"
          command: "bash {{ hadoop_dir }}/start-deamons.sh"

        - name: Run jps and capture the output
          command: jps
          register: jps_output

        - name: Verify that all required JVM processes are running
          assert:
            that:
              - "'NodeManager' in jps_output.stdout"
              - "'ResourceManager' in jps_output.stdout"
            fail_msg: "One or more required JVM processes are not running"
            success_msg: "All required JVM processes are running"

    - name: Pull HiBench repo
      block:
        - name: Remove the existing repository if present
          file:
            path: "{{ hibench_dir }}"
            state: absent

        - name: Create directory for HiBench
          file:
            path: "{{ hibench_dir }}"
            state: directory
            mode: '0755'
            owner: "{{ linux_username }}"
            group: "{{ linux_username }}"
            recurse: yes

        - name: Pull HiBench repository
          become_user: "{{ linux_username }}" # otherwise we get ownership issues
          git:
            repo: https://github.com/Intel-bigdata/HiBench.git
            dest: "{{ hibench_dir }}"
            version: "{{ hibench_version }}"
            force: yes
            update: yes

        - name: Add maven archives to cache
          synchronize:
            src: "{{ lookup('env', 'PWD') }}/.m2.repository.cache.maven-download-plugin"
            dest: "{{ maven_cache_download_plugin_dir }}"
            recursive: true
