---
- name: Add a new user with sudo privileges
  hosts: remote
  vars_files:
    - ../vars.yaml
  become: true
  tasks:
    - name: Create a new user
      user:
        name: "{{ linux_username }}"
        password: "{{ linux_password | password_hash('sha512') }}"
        shell: /bin/bash
        state: present
        create_home: yes
        groups:
          - sudo
        append: yes

    - name: Allow new user to have passwordless sudo
      lineinfile:
        dest: /etc/sudoers
        state: present
        regexp: '^%{{ linux_username }}'
        line: '%{{ linux_username }} ALL=(ALL) NOPASSWD: ALL'
        validate: 'visudo -cf %s'

- name: Install Ansible/pip dependencies on leader machine
  hosts: leader
  vars_files:
    - ../vars.yaml
  become: true
  tasks:
    - name: Check if pip is installed
      command: python3 -m pip -V
      register: pip_installed
      ignore_errors: true

    - name: Download pip installer
      get_url:
        url: https://bootstrap.pypa.io/get-pip.py
        dest: /tmp/get-pip.py
        mode: '0644'
      when: pip_installed.failed

    - name: Run pip installer
      command: python3 /tmp/get-pip.py --user
      when: pip_installed.failed

    - name: Ensure python3-venv is installed
      apt:
        name: python3-venv
        state: present
      when: ansible_os_family == 'Debian'

    - name: Install packaging package
      pip:
        name:
          - packaging
          - virtualenv

    - name: Create virtual environment
      become_user: root
      command:
        cmd: python3 -m venv {{ venv_path }}
        creates: "{{ venv_path }}"

    - name: Install pre-requisites
      pip:
        name:
          - openshift
          - pyyaml
          - kubernetes
        virtualenv: "{{ venv_path }}"
        virtualenv_site_packages: true

- name: Check kubectl is present on leader node
  hosts: leader
  vars_files:
    - ../vars.yaml
  become: yes
  tasks:
    - name: Check if kubectl is installed and connected to cluster
      command: kubectl cluster-info
      register: kubectl_installed
      ignore_errors: true

    - name: Fail playbook
      fail:
        msg: kubectl not installed or not connected with a cluster on leader node. Can't continue playbook without a present k8s deployment.
      when: kubectl_installed.rc != 0

    - name: Ensure .kube directory exists for user {{ linux_username }}
      file:
        path: /home/{{ linux_username }}/.kube
        state: directory
        owner: "{{ linux_username }}"
        group: "{{ linux_username }}"
        mode: '0755'

    - name: Copy kubeconfig to {{ linux_username }}'s home directory
      copy:
        remote_src: true
        src: /etc/kubernetes/admin.conf
        dest: /home/{{ linux_username }}/.kube/config
        owner: "{{ linux_username }}"
        group: "{{ linux_username }}"
        mode: '0644'

- name: Install helm on leader node
  hosts: leader
  vars_files:
    - ../vars.yaml
  tasks:
    - name: Check if helm is installed
      command: which helm
      register: helm_installed
      ignore_errors: true

    - name: Download Helm installation script
      get_url:
        url: https://raw.githubusercontent.com/helm/helm/master/scripts/get
        dest: /tmp/get_helm.sh
        mode: '0755'
      when: helm_installed.failed
      become: yes

    - name: Install Helm
      command: /tmp/get_helm.sh -v "v{{ helm_version }}"
      when: helm_installed.failed
      become: yes

    - name: Remove Helm installation script
      file:
        path: /tmp/get_helm.sh
        state: absent
      when: helm_installed.failed
      become: yes

# to be on the save side and prevent any manual managing effort, we'll create more PV then actually required
- name: Create bind-mount directories for PersistentVolumes on all nodes
  hosts: remote
  become: yes
  vars_files:
    - ../vars.yaml
  tasks:
    - name: Create MongoDB directory
      file:
        path: "/mnt/disks/mongodb-data-{{ ansible_hostname | lower }}"
        state: directory
        mode: '0755'
      loop: "{{ range(1, replica_count + 1) }}"

    - name: Create Zookeeper data source directory if it does not exist
      file:
        path: "/mnt/disks/zookeeper-data-{{ ansible_hostname | lower }}"
        state: directory
        mode: '0755'

    - name: Create HDFS data source directory if it does not exist
      file:
        path: "/mnt/disks/hdfs-data-{{ ansible_hostname | lower }}-{{ item }}"
        state: directory
        mode: '0755'
      loop: "{{ range(1, data_node_replicas + name_node_replicas + journal_node_replicas + 1) }}"

    - name: Create MongoDB target directory if it does not exist
      file:
        path: "/Users/{{ linux_username }}/mongodb-data-{{ ansible_hostname | lower }}"
        state: directory
        mode: '0755'
      loop: "{{ range(1, replica_count + 1) }}"

    - name: Create Zookeeper data target directory if it does not exist
      file:
        path: "/Users/{{ linux_username }}/zookeeper-data-{{ ansible_hostname | lower }}"
        state: directory
        mode: '0755'

    - name: Create HDFS data target directory if it does not exist
      file:
        path: "/Users/{{ linux_username }}/hdfs-data-{{ ansible_hostname | lower }}-{{ item }}"
        state: directory
        mode: '0755'
      loop: "{{ range(1, data_node_replicas + name_node_replicas + journal_node_replicas + 1) }}"

    - name: Bind mount MongoDB directories
      ansible.posix.mount:
        src: "/Users/{{ linux_username }}/mongodb-data-{{ ansible_hostname | lower }}"
        path: "/mnt/disks/mongodb-data-{{ ansible_hostname | lower }}"
        fstype: none
        opts: bind
        state: mounted
      loop: "{{ range(1, replica_count + 1) }}"

    - name: Bind mount zookeeper directories
      ansible.posix.mount:
        src: "/Users/{{ linux_username }}/zookeeper-data-{{ ansible_hostname | lower }}"
        path: "/mnt/disks/zookeeper-data-{{ ansible_hostname | lower }}"
        fstype: none
        opts: bind
        state: mounted

    - name: Bind mount hdfs directories
      ansible.posix.mount:
        src: "/Users/{{ linux_username }}/hdfs-data-{{ ansible_hostname | lower }}-{{ item }}"
        path: "/mnt/disks/hdfs-data-{{ ansible_hostname | lower }}-{{ item }}"
        fstype: none
        opts: bind
        state: mounted
      loop: "{{ range(1, data_node_replicas + name_node_replicas + journal_node_replicas + 1) }}"

- name: Template HDFS cluster charts
  hosts: leader
  vars_files:
    - ../vars.yaml
  vars:
    local_charts_dir: "{{ lookup('env', 'PWD') }}/ansible/charts/"
    remote_charts_dir: /Users/{{ linux_username }}/tmp/charts
  become: true
  tasks:
    - name: Create a temporary directory for templating
      file:
        path: "{{ remote_charts_dir }}"
        state: directory
        mode: '0755'

    - name: Template the Zookeeper cluster file
      template:
        src: "{{ local_charts_dir }}/zookeeper-cluster.yaml"
        dest: "{{ remote_charts_dir }}/zookeeper-cluster.yaml"

    - name: Template the Zookeeper ZNode file
      template:
        src: "{{ local_charts_dir }}/zookeeper-znode.yaml"
        dest: "{{ remote_charts_dir }}/zookeeper-znode.yaml"

    - name: Template the HDFS cluster file
      template:
        src: "{{ local_charts_dir }}/hdfs-cluster.yaml"
        dest: "{{ remote_charts_dir }}/hdfs-cluster.yaml"

    - name: Copy webhdfs.yaml to leader
      template:
        src: "{{ lookup('env', 'PWD') }}/ansible/charts/webhdfs.yaml"
        dest: "{{ remote_charts_dir }}/webhdfs.yaml"

- name: Deploy spark-operator helm chart
  hosts: leader
  vars_files:
    - ../vars.yaml
  become: yes
  tasks:
    - name: Add spark-operator repository
      kubernetes.core.helm_repository:
        name: spark-operator
        repo_url: https://kubeflow.github.io/spark-operator

    - name: Deploy spark-operator helm chart
      kubernetes.core.helm:
        name: spark-operator
        chart_ref: spark-operator/spark-operator
        chart_version: "{{ spark_operator_version }}"
        release_namespace: "{{ k8s_namespace }}"
        atomic: true
        wait: true

- name: Deploy MongoDB replica set
  hosts: leader
  vars_files:
    - ../vars.yaml
  become: yes
  tasks:
    - name: Deploy mongodb helm chart
      kubernetes.core.helm:
        name: mongodb
        chart_ref: oci://registry-1.docker.io/bitnamicharts/mongodb
        chart_version: "{{ mongo_db_chart_version }}"
        release_namespace: "{{ k8s_namespace }}"
        atomic: true
        wait: true
        values:
          architecture: replicaset
          replicaSetName: "{{ replica_set_name }}"
          persistence:
            storageClass: "{{ storage_class }}"
          replicaCount: "{{ replica_count }}"
          auth:
            rootUser: "{{ mongodb_root_username }}"
            usernames:
              - "{{ mongodb_username }}"
            passwords:
              - "{{ mongodb_password }}"
            databases:
              - "{{ mongodb_database }}"
          arbiter:
            resourcesPreset: "{{ mongodb_resource_preset }}"

- name: Install HDFS Operator's
  hosts: leader
  vars_files:
    - ../vars.yaml
  vars:
    ansible_python_interpreter: "{{ venv_path }}/bin/python3"
  become: yes
  become_user: root
  tasks:
    - name: Add stackable-stable repository
      kubernetes.core.helm_repository:
        name: stackable-stable
        repo_url: https://repo.stackable.tech/repository/helm-stable/

    - name: Deploy Zookeeper operator helm chart
      kubernetes.core.helm:
        name: zookeeper-operator
        chart_ref: stackable-stable/zookeeper-operator
        chart_version: "{{ stackable_operator_version }}"
        release_namespace: "{{ k8s_namespace }}"
        atomic: true
        wait: true

    - name: Deploy HDFS operator helm chart
      kubernetes.core.helm:
        name: hdfs-operator
        chart_ref: stackable-stable/hdfs-operator
        chart_version: "{{ stackable_operator_version }}"
        release_namespace: "{{ k8s_namespace }}"
        atomic: true
        wait: true

    - name: Deploy Commons operator helm chart
      kubernetes.core.helm:
        name: commons-operator
        chart_ref: stackable-stable/commons-operator
        chart_version: "{{ stackable_operator_version }}"
        release_namespace: "{{ k8s_namespace }}"
        atomic: true
        wait: true

    - name: Deploy Secret operator helm chart
      kubernetes.core.helm:
        name: secret-operator
        chart_ref: stackable-stable/secret-operator
        chart_version: "{{ stackable_operator_version }}"
        release_namespace: "{{ k8s_namespace }}"
        atomic: true
        wait: true

    - name: Deploy Listener operator helm chart
      kubernetes.core.helm:
        name: listener-operator
        chart_ref: stackable-stable/listener-operator
        chart_version: "{{ stackable_operator_version }}"
        release_namespace: "{{ k8s_namespace }}"
        atomic: true
        wait: true

- name: Deploy HDFS cluster charts
  hosts: leader
  vars_files:
    - ../vars.yaml
  vars:
    remote_charts_dir: /Users/{{ linux_username }}/tmp/charts
    ansible_python_interpreter: "{{ venv_path }}/bin/python3"
  become: yes
  tasks:
    - name: Deploy Zookeeper chart
      kubernetes.core.k8s:
        name: zookeeper
        src: "{{ remote_charts_dir }}/zookeeper-cluster.yaml"
        namespace: "{{ k8s_namespace }}"
        state: present
        wait: true

    - name: Deploy Zookeeper ZNode chart
      kubernetes.core.k8s:
        name: zookeeper-znode
        src: "{{ remote_charts_dir }}/zookeeper-znode.yaml"
        namespace: "{{ k8s_namespace }}"
        state: present
        wait: true

    - name: Deploy HDFS chart
      kubernetes.core.k8s:
        name: hdfs
        src: "{{ remote_charts_dir }}/hdfs-cluster.yaml"
        namespace: "{{ k8s_namespace }}"
        state: present
        wait: true

    - name: Deploy WebHDFS application
      kubernetes.core.k8s:
        name: webhdfs
        namespace: "{{ k8s_namespace }}"
        src: "{{ remote_charts_dir }}/webhdfs.yaml"
        state: present
        wait: true

- name: Wait for HDFS to be ready
  hosts: leader
  vars_files:
    - ../vars.yaml
  vars:
    ansible_python_interpreter: "{{ venv_path }}/bin/python3"
  become: yes
  tasks:
    - name: Verify HDFS NameNode pods are ready
      k8s_info:
        namespace: "{{ k8s_namespace }}"
        kind: Pod
        label_selectors:
          - 'role=namenode'
      register: namenode_pods
      retries: 30
      delay: 10
      until: namenode_pods.resources | map(attribute='status.containerStatuses') | map('map', attribute='ready') | flatten | reject('equalto', true) | list | length == 0

    - name: Verify HDFS DataNode pods are ready
      k8s_info:
        namespace: "{{ k8s_namespace }}"
        kind: Pod
        label_selectors:
          - 'role=datanode'
      register: datanode_pods
      retries: 30
      delay: 10
      until: datanode_pods.resources | map(attribute='status.containerStatuses') | map('map', attribute='ready') | flatten | reject('equalto', true) | list | length == 0

- name: Copy custom SparkListener to HDFS
  hosts: leader
  become: yes
  vars_files:
    - ../vars.yaml
  vars:
    ansible_python_interpreter: "{{ venv_path }}/bin/python3"
    temp_dir: "/Users/{{ linux_username }}/tmp"
    remote_charts_dir: "{{ temp_dir }}/charts"
  tasks:
    - name: Copy the JAR file to remote machine
      synchronize:
        src: "{{ lookup('env', 'PWD') }}/ansible/jars/{{ listener_jar }}"
        dest: "{{ temp_dir }}/{{ listener_jar }}"
        delete: yes

    - name: Copy jar local file to webhdfs pod
      kubernetes.core.k8s_cp:
        namespace: "{{ k8s_namespace }}"
        pod: webhdfs-0
        local_path: "{{ temp_dir }}/{{ listener_jar }}"
        remote_path: "/tmp/{{ listener_jar }}"

    - name: Issue initial PUT command to create file in HDFS
      kubernetes.core.k8s_exec:
        namespace: "{{ k8s_namespace }}"
        pod: webhdfs-0
        command: "curl -s -XPUT -T /tmp/{{ listener_jar }} \"http://{{ hdfs_cluster_name }}-namenode-default-0.{{ hdfs_cluster_name }}-namenode-default.default.svc.cluster.local:9870/webhdfs/v1/{{ listener_jar }}?user.name=stackable&op=CREATE&noredirect=true\""
      register: put_result

    - name: Extract location URL from PUT response
      set_fact:
        location_url: "{{ put_result.stdout | from_json() | json_query('Location') }}"

    - name: Issue second PUT command to complete file creation
      kubernetes.core.k8s_exec:
        namespace: "{{ k8s_namespace }}"
        pod: webhdfs-0
        command: "curl -s -XPUT -T /tmp/{{ listener_jar }} {{ location_url }}"

    - name: Recheck status to verify file creation
      kubernetes.core.k8s_exec:
        namespace: "{{ k8s_namespace }}"
        pod: webhdfs-0
        command: "curl -s -XGET \"http://{{ hdfs_cluster_name }}-namenode-default-0.{{ hdfs_cluster_name }}-namenode-default.default.svc.cluster.local:9870/webhdfs/v1/?op=LISTSTATUS\""
      register: status_result

    - name: Parse status_result stdout as JSON
      set_fact:
        file_status: "{{ status_result.stdout | from_json }}"

    - name: Verify the file_status contains the created file
      assert:
        that:
          - file_status.FileStatuses.FileStatus | length > 0
          - file_status.FileStatuses.FileStatus[0].pathSuffix == "{{ listener_jar }}"
          - file_status.FileStatuses.FileStatus[0].length > 0

    - name: Delete WebHDFS application
      kubernetes.core.k8s:
        name: webhdfs
        src: "{{ remote_charts_dir }}/webhdfs.yaml"
        namespace: "{{ k8s_namespace }}"
        state: absent
        wait: true
