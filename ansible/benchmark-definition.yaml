# Data scale profile. Available value is tiny, small, large, huge, gigantic and bigdata.
# The definition of these profiles can be found in the workload's conf file,
# i.e. https://github.com/Intel-bigdata/HiBench/blob/master/conf/workloads/micro/wordcount.conf
scale: tiny # {tiny, small, large, huge, gigantic, bigdata}

# Which workloads do you want to run?
workloads:
#  # MICRO
  - { name: "repartition", enabled: true, category: "micro" }
  - { name: "sleep", enabled: true, category: "micro" }
  - { name: "sort", enabled: true, category: "micro" }
  - { name: "terasort", enabled: true, category: "micro" }
  - { name: "wordcount", enabled: true, category: "micro" }
#  # MACHINE LEARNING
  - { name: "als", enabled: true, category: "ml" }
  - { name: "bayes", enabled: true, category: "ml" }
  - { name: "gbt", enabled: true, category: "ml" }
  - { name: "gmm", enabled: true, category: "ml" }
  - { name: "kmeans", enabled: true, category: "ml" }
  - { name: "lda", enabled: true, category: "ml" }
  - { name: "linear", enabled: false, category: "ml" } # requires way more memory than other jobs!
  - { name: "lr", enabled: true, category: "ml" }
  - { name: "pca", enabled: true, category: "ml" }
  - { name: "rf", enabled: true, category: "ml" }
  - { name: "svd", enabled: true, category: "ml" }
  - { name: "svm", enabled: true, category: "ml" }
#  # WEBSEARCH
  - { name: "pagerank", enabled: true, category: "websearch" }
#  # GRAPH
  - { name: "nweight", enabled: true, category: "graph" }
  - { name: "pagerank", enabled: true, category: "graph" }

# configuration
is_adaptive: true
skip_data_generation: true
bridge_service_address: "tcp://{{ hostvars['leaderNode']['ip'] }}:5555"
driver_memory: 8192m
executor_memory: 8192m
target_runtime: 50000
initial_executors: 2
min_executors: 1
max_executors: 3
