# Data scale profile. Available value is tiny, small, large, huge, gigantic and bigdata.
# The definition of these profiles can be found in the workload's conf file,
# i.e. https://github.com/Intel-bigdata/HiBench/blob/master/conf/workloads/micro/wordcount.conf
scale: small # {tiny, small, large, huge, gigantic, bigdata}

# Which workloads do you want to run?
workloads:
  # MICRO
  - { name: "repartition", enabled: false, category: "micro" }
  - { name: "sleep", enabled: false, category: "micro" }
  - { name: "sort", enabled: false, category: "micro" }
  - { name: "terasort", enabled: false, category: "micro" }
  - { name: "wordcount", enabled: false, category: "micro" }
  # MACHINE LEARNING
  - { name: "als", enabled: true, category: "ml" }
  - { name: "bayes", enabled: false, category: "ml" }
  - { name: "gbt", enabled: false, category: "ml" }
  - { name: "gmm", enabled: false, category: "ml" }
  - { name: "kmeans", enabled: true, category: "ml" }
  - { name: "lda", enabled: false, category: "ml" }
  - { name: "linear", enabled: false, category: "ml" }
  - { name: "lr", enabled: false, category: "ml" }
  - { name: "pca", enabled: false, category: "ml" }
  - { name: "rf", enabled: false, category: "ml" }
  - { name: "svd", enabled: false, category: "ml" }
  - { name: "svm", enabled: false, category: "ml" }
  # WEBSEARCH
  - { name: "pagerank", enabled: false, category: "websearch" }
  # GRAPH
  - { name: "nweight", enabled: false, category: "graph" }
  - { name: "pagerank", enabled: false, category: "graph" }

# configuration
is_adaptive: true
bridge_service_address: "tcp://{{ hostvars['leaderNode']['ip'] }}:5555"
driver_memory: 8192m
executor_memory: 8192m
target_runtime: 30000
initial_executors: 1
min_executors: 1
max_executors: 10
