apiVersion: "sparkoperator.k8s.io/v1beta2"
kind: SparkApplication
metadata:
  name: {{ name }}
spec:
  type: Scala
  mode: cluster
  image: {{ spark_image }}
  imagePullPolicy: IfNotPresent
  mainClass: {{ hibench_main_class }}
  mainApplicationFile: {{ hibench_jar }}
  arguments:
    {% for argument in arguments -%}
    - "{{ argument }}"
    {% endfor %}
  sparkVersion: "{{ spark_version }}"
  restartPolicy:
    type: Never
  monitoring:
    exposeDriverMetrics: true
    exposeExecutorMetrics: true
  driver:
    cores: {{ driver_cores }}
    memory: "{{ driver_memory }}"
    labels:
      version: "{{ spark_version }}"
    serviceAccount: spark-operator-spark
    configMaps:
      - name: hibench-config
        path: /mnt/config-maps
    env:
      - name: SPARKBENCH_PROPERTIES_FILES
        value: /mnt/config-maps/hibench.conf
  executor:
    cores: {{ executor_cores }}
    memory: "{{ executor_memory }}"
    labels:
      version: "{{ spark_version }}"
    configMaps:
      - name: hibench-config
        path: /mnt/config-maps
    env:
      - name: SPARKBENCH_PROPERTIES_FILES
        value: /mnt/config-maps/hibench.conf
  dynamicAllocation:
    enabled: True
    # initially forces Spark to not create an executor, so we can use the initial scale-out recommendation
    minExecutors: 0
  deps:
    jars:
      - {{ listener_jar }}
  sparkConf:
    # listener conf
    "spark.extraListeners": "de.tu_berlin.jarnhold.listener.CentralizedSparkListener"
    "spark.customExtraListener.isAdaptive": "{{ is_adaptive }}"
    "spark.customExtraListener.bridgeServiceAddress": "{{ bridge_service_address }}"
    "spark.customExtraListener.minExecutors": "{{ min_executors }}"
    "spark.customExtraListener.maxExecutors": "{{ max_executors }}"
    # listener app spec
    "spark.customExtraListener.datasizeMb": "{{ datasize_mb }}"
    "spark.customExtraListener.targetRuntime": "{{ target_runtime }}"
    # listener environment spec
    "spark.customExtraListener.env.hadoopVersion": "{{ hadoop_version }}"
    "spark.customExtraListener.env.sparkVersion": "{{ spark_version }}"
    "spark.customExtraListener.env.scalaVersion": "{{ scala_version }}"
    "spark.customExtraListener.env.javaVersion": "{{ java_version }}"
    # settings that allow for failure injection
    "spark.network.timeout": "20s"
    "spark.network.timeoutInterval": "20s"
    "spark.storage.blockManagerHeartbeatTimeoutMs": "20000ms"
    "spark.storage.blockManagerSlaveTimeoutMs": "20000ms"
    "spark.storage.blockManagerTimeoutIntervalMs": "20000ms"
    "spark.executor.heartbeatInterval": "10s"
    "spark.scheduler.excludeOnFailure.unschedulableTaskSetTimeout": "20s"
    # set max resultsize to unlimited
    "spark.driver.maxResultSize": "0"
    # prevent spark dynamic allocation to actually work (we only want shuffle-tracking)
    "spark.scheduler.minRegisteredResourcesRatio": "1.0"
    "spark.dynamicAllocation.shuffleTracking.enabled": "true"
    "spark.decommission.enabled": "true"
    "spark.storage.decommission.shuffleBlocks.enabled": "true"
    "spark.dynamicAllocation.schedulerBacklogTimeout": "10d"
    "spark.dynamicAllocation.executorIdleTimeout": "60s"
    "spark.kubernetes.executor.checkAllContainers": "true"
    "spark.kubernetes.executor.deleteOnTermination": "true"
    # metric monitoring + k8s monitoring
    "spark.ui.prometheus.enabled": "true"
    "spark.metrics.appStatusSource.enabled": "true"
    "spark.metrics.staticSources.enabled": "true"
    "spark.metrics.executorMetricsSource.enabled": "true"
    "spark.executor.processTreeMetrics.enabled": "true"
    "spark.kubernetes.driver.annotation.prometheus.io/scrape": "true"
    "spark.kubernetes.driver.annotation.prometheus.io/path": "/metrics/prometheus/ | /metrics/executors/prometheus/"
    "spark.kubernetes.driver.annotation.prometheus.io/port": "4040"
